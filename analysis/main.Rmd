---
title: "FS20C4 - Steinschlag"
author: "Fernando Millan Villalobos"
date: "February 2020"
output:
  html_document:
    code_folding: show
    echo: TRUE
    warning: FALSE
    message: FALSE
    theme: paper
    df_print: kable
    toc: yes
    toc_depth: 4
    number_sections: yes
    toc_float: 
      collapsed: yes
      smooth_scroll: false
---

```{r config git r version cpus, echo=FALSE}
# CONFIG
# user_name <- "nando" # your Git username (only needed if
# you want to deploy to GH pages)
# project_name <- "rddj-template" # adapt!
package_date <- "2020-04-01" # date of the CRAN snapshot that
# the checkpoint package uses
r_version <- "4.0.0" # R-Version to use
options(Ncpus = 4) # use 4 cores for parallelized installation of packages
if (r_version != paste0(version$major, ".", version$minor)) {
  stop("ERROR: specified R version does not match currently used.")
}
```

# Notes

This report was generated on `r Sys.time()`. R version: `r paste0(version$major, ".", version$minor)` on `r version$platform`. For this report, CRAN packages as of `r package_date` were used.

## R-Script & data

The preprocessing and analysis of the data was conducted in the [R project for statistical computing](https://www.r-project.org/). The RMarkdown script used to generate this document and all the resulting data can be downloaded [under this link](http://`r user_name`.github.io/`r project_name`/rscript.zip). Through executing `main.Rmd`, the herein described process can be reproduced and this document can be generated. In the course of this, data from the folder `ìnput` will be processed and results will be written to `output`. 

## GitHub

The code for the herein described process can also be freely downloaded from [https://github.com/`r user_name`/`r project_name`](https://github.com/`r user_name`/`r project_name`). 

## Data description of output files

### `out_1.csv`

| Datum      | Uhrzeit | Masse [kg] | Geschwindigkeit [m/s]
|------------|---------|-----------------------------------------------------------------------------|
| 2019-01-01 | 09:00   | 194        | 8.4
| 2019-01-01 | 21:00   | 224        | 8.8
| 2019-01-02 | 14:00   | 3114       | 9.2

# Set up

```{r packages set up wd scientific notation, echo=FALSE}
detach_all_packages <- function() {
  basic_packages_blank <-  c("stats",
                             "graphics",
                             "grDevices",
                             "utils",
                             "datasets",
                             "methods",
                             "base")
  basic_packages <- paste("package:", basic_packages_blank, sep = "")

  package_list <- search()[
    ifelse(unlist(gregexpr("package:", search())) == 1, TRUE, FALSE)]

  package_list <- setdiff(package_list, basic_packages)

  if (length(package_list) > 0)  for (package in package_list) {
    detach(package, character.only = TRUE, unload = TRUE)
    print(paste("package ", package, " detached", sep = ""))
  }
}

detach_all_packages()

# this allows multiple persons to use the same RMarkdown
# without adjusting the working directory by themselves all the time
source("scripts/csf.R")
path_to_wd <- csf() # if this - for some reason - does not work,
# replace with a hardcoded path, like so: "~/projects/rddj-template/analysis/"
if (is.null(path_to_wd) | !dir.exists(path_to_wd)) {
  print("WARNING: No working directory specified for current user")
} else {
  setwd(path_to_wd)
}

# suppress scientific notation
options(scipen = 999)

# unload global rstudioapi and knitr again to avoid conflicts with checkpoint
# this is only necessary if executed within RStudio
# outside of RStudio, namely in the knit.sh script, this causes RMarkdown
# rendering to fail, thus should not be executed there
if (Sys.getenv("RSTUDIO") == "1") {
  detach_all_packages()
}
```

## Define packages

```{r define packages, echo=TRUE, message=FALSE, warning=FALSE}
# from https://mran.revolutionanalytics.com/web/packages/\
# checkpoint/vignettes/using-checkpoint-with-knitr.html
# if you don't need a package, remove it from here (commenting not sufficient)
# tidyverse: see https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/
cat("
library(rstudioapi)
library(tidyverse) # ggplot2, dplyr, tidyr, readr, purrr, tibble, magrittr, readxl
library(scales) # scales for ggplot2
library(jsonlite) # json
library(MASS)
library(lintr) # code linting
library(sf) # spatial data handling
library(rmarkdown)
library(cowplot) # theme
library(extrafont)", # fonts
file = "manifest.R")
```

## Install packages

```{r install packages, echo=TRUE, message=FALSE, warning=FALSE}
# if checkpoint is not yet installed, install it (for people using this
# system for the first time)
if (!require(checkpoint)) {
  if (!require(devtools)) {
    install.packages("devtools", repos = "http://cran.us.r-project.org")
    require(devtools)
  }
  devtools::install_github("RevolutionAnalytics/checkpoint",
                           ref = "v0.3.2", # could be adapted later,
                           # as of now (beginning of July 2017
                           # this is the current release on CRAN)
                           repos = "http://cran.us.r-project.org")
  require(checkpoint)
}
# nolint start
if (!dir.exists("~/.checkpoint")) {
  dir.create("~/.checkpoint")
}
# nolint end
# install packages for the specified CRAN snapshot date
checkpoint(snapshotDate = package_date,
           project = path_to_wd,
           verbose = T,
           scanForPackages = T,
           use.knitr = F,
           R.version = r_version)
rm(package_date)
```

## Load packages

```{r load packages sessionInfo, echo=TRUE, message=FALSE, warning=FALSE}
source("manifest.R")
unlink("manifest.R")
sessionInfo()
```

## Load additional scripts

```{r additional scripts, echo=TRUE, message=FALSE, warning=FALSE}
# if you want to outsource logic to other script files, see README for 
# further information
# Load all visualizations functions as separate scripts
knitr::read_chunk("scripts/dviz.supp.R")
source("scripts/dviz.supp.R")
knitr::read_chunk("scripts/themes.R")
source("scripts/themes.R")
knitr::read_chunk("scripts/plot_grid.R")
source("scripts/plot_grid.R")
knitr::read_chunk("scripts/align_legend.R")
source("scripts/align_legend.R")
knitr::read_chunk("scripts/label_log10.R")
source("scripts/label_log10.R")
```

# Introduction
This notebook is thought as a working document for the development and documentation of the [FS20C4 - Steinschlag project](https://ds-spaces.technik.fhnw.ch/app/uploads/sites/17/2019/09/hs19c4steinschlag-3.pdf)). 

As requested by the village of Schiers (GR), a cursory review of the potential rockfall hazard was performed. This study was conducted to assist the village in determining potential rockfall and setting mitigation actions that can be useful to avoid closure of a section of the roadway that connects Schiers with the outside world. The investigation consisted of a data analysis and a probability calculation carried on by a hired data scientist. The area of study covered the natural bedrock outcrops above the roadway. Protection methods are put into play once rocks have started to destabilize. Typically, rockfall events are mitigated in one of two ways: either passive or active mitigation. In the case of passive mitigations, these cover only the effects of the rockfall and they are generally used in the deposition or run-out zones. Using mesh cable nets to catch falling rocks and contain them as to avoid any damage to the area below represent such a scenario. These methods, also called mitigation efforts, are directed towards avoidance, stabilization, protection and management of risk.

# Data Importing
Originally two different datasets containing rockfalls data (day, hour, mass and speed), one per mountain side, were provided for our analysis [Dataset 1](https://www.dropbox.com/s/i58gdv6pzi03rhr/out_1.csv?dl=0), [Dataset 2](https://www.dropbox.com/s/3nk9pv7nzz8f0qb/out_2.csv?dl=0). More data were necessary to know the traffic density within 24 hours in the area. This data come from the report [__Mikrozensus Mobilität und Verkehr 2015__](https://www.bfs.admin.ch/bfs/de/home/statistiken/mobilitaet-verkehr/personenverkehr/verkehrsverhalten.assetdetail.1840604.html) by the Bundesamt für Statistik (BFS). We must point out that the density traffic data per hour available were collected and summarized at national level not at local level. The table with the used data can be found [here](https://www.bfs.admin.ch/bfs/de/home/statistiken/mobilitaet-verkehr/personenverkehr/verkehrsverhalten/tabellen-2015/hauptbericht.assetdetail.2500421.html).  

```{r importing data}
# Import datasets
data01 <- read_csv("input/out_1.csv")
data02 <- read_csv("input/out_2.csv")
data_traffic <- read_csv2("input/tagesverlauf_verkehrsmittel_2015.csv")

# Select columns and rows we are interesting in with values different as na´s
(data01 <- data01 %>% 
  select(1:4) %>% 
  na.omit())

(data02 <- data02 %>% 
  select(1:4) %>% 
  na.omit())

(data_traffic <- data_traffic %>% 
  select(c(1, 8)))

```

# Data Pre-Processing
In this step we are going to transform our imported data in a way that allows us to apply further analysis. Typically, this process requires some data wrangling, data transformation and the creation of new variables.

```{r renaming cols parseHour parsing hours}
# Rename columns
names(data01) <- c("date", "time", "mass", "speed")
names(data02) <- c("date", "time", "mass", "speed")
names(data_traffic) <- c("hour", "pct_traffic")

# Parse hour column
hours <- str_split_fixed(data_traffic$hour, " ", n = 2)[, 1] # get the hours interval
hour <- str_split_fixed(hours, "-", n = 2)[, 1] # get the hours starting at 0h.
data_traffic[, 1] <- hour # replace old values with new ones
(data_traffic[, 1] <- as.integer(data_traffic$hour)) # parse as integer

# Calculating kinetic energy (kJ)
ke <- function(m, s) {
  x <- m
  y <- s
  return(round(.5 * x * y ** 2 / 1000, 5))
}
data01$energy_kJ <- ke(data01$mass, data01$speed) # adding variable energy to data01
data02$energy_kJ <- ke(data02$mass, data02$speed) # adding variable energy to data02

# Checking the data

# Checking if there´s any NA
sum(is.na(data01)) 
sum(is.na(data02))

# A quick look at the dataframe let us spot a mass value equal to 0 in dataset 02. Let´s check it for both datasets. 
sum(data01$mass == 0)
sum(data02$mass == 0)

# It seems there´s a mass value equal to 0 in dataset 02. As we know that a rock muss have certain weight,
# we get rid off this value and keep the rest of the data.
data02 <-subset(data02, mass != 0)
sum(data02$mass == 0)

# Now we turn our eyes to the data_traffic dataset.
sum(data_traffic$pct_traffic)

# Traffic_density data is scaled to 216.8 percent and needs to be scaled down to 100 percent
data_traffic <- mutate(data_traffic, pct_traffic = pct_traffic / 2.168)
sum(data_traffic$pct_traffic)

```

# Exploratory Data Analysis (EDA)
## Data Summarization
Now we want to describe some important properties of the distribution of the values across the observations in our datasets, an overview of the key properties of the data.

```{r eda summarising}
# Finding the most common value of our different variables in our two samples.
summary(data01)
summary(data02)

# Adding  the variance, the sample standard deviation and the inter-quartile range (IQR).
select(data01, c(mass, speed, energy_kJ)) %>% 
  summarise_each(funs(var, sd, IQR))

select(data02, c(mass, speed, energy_kJ)) %>% 
  summarise_each(funs(var, sd, IQR))
```

## Data Visualization
We´re going to explore and gain insights from our data using one of the most outstanding ability of human beings: capturing visual patterns. Let´s explore distributions of variables in our two datasets: mass, speed and energy.

### Mass Distribution
As we can see in the chart, the most common rockfalls from our dataset 01 are the ones that involves rocks of 500Kg. or less. Events of rock than more than 1500Kg. are rare. Regarding dataset 02, we notice that falling rocks are significant lighter than in the opposite slope. Most common rockfalls are up to 100Kg.

```{r eda visualization plotting mass}

# Chart data01
p1 <- ggplot(data = data01, aes(x = mass)) + 
  geom_histogram(bins = 30, fill = "#22577a") +
  scale_y_continuous(name = "Count", expand = c(0, 0)) +
  scale_x_continuous(name = "Mass (Kg)", breaks = c(seq(0, 3500, 500)), expand = c(0, 0)) + 
  labs(title = "Mass distribution in Dataset 01") +
  coord_cartesian(clip = "off") +
  theme_dviz_hgrid() +
  theme(
    axis.line.x = element_blank(),
    plot.margin = margin(3, 7, 3, 1.5)
  )

# Chart data02
p2 <- ggplot(data = data02, aes(x = mass)) + 
  geom_histogram(bins = 30, fill = "#22577a") +
  scale_y_continuous(name = "Count", expand = c(0, 0)) +
  scale_x_continuous(name = "Mass (Kg)", breaks = c(seq(0, 550, 100)), expand = c(0, 0)) + 
  labs(title = "Mass distribution in Dataset 02") +
  coord_cartesian(clip = "off") +
  theme_dviz_hgrid() +
  theme(
    axis.line.x = element_blank(),
    plot.margin = margin(3, 7, 3, 1.5)
  )

# Plot datasets side by side to ease an overview
plot_grid(
  p1, NULL, p2,
  NULL, NULL, NULL,
  align = 'hv',
  rel_widths = c(1, .04, 1),
  rel_heights = c(1, .04, 1)
)

# Now we want to determine to what extent the observed data points do or do not follow a given distribution. We do that for our both datasets available.

# For data01
expo_fit_mass_data01 <- fitdistr(data01$mass, "exponential")
gamma_fit_mass_data01 <- fitdistr(data01$mass, "gamma")
lognorm_fit_mass_data01 <- fitdistr(data01$mass, "lognormal")

# For data02
expo_fit_mass_data02 <- fitdistr(data02$mass, "exponential")
gamma_fit_mass_data02 <- fitdistr(data02$mass, "gamma")
lognorm_fit_mass_data02 <- fitdistr(data02$mass, "lognormal")

# Q-Q Plots

# Q-Q Plot Data01
p3 <- ggplot(data01, aes(sample = mass)) + 
  geom_abline(slope = 1, intercept = 0, color = "grey70") +
  stat_qq(distribution = qexp,
          dparams = list(expo_fit_mass_data01$estimate[1]),
          geom = "line",
          aes(color = "exponential"),
          size = .8) +
  stat_qq(distribution = qgamma,
          dparams = list(gamma_fit_mass_data01$estimate[1], gamma_fit_mass_data01$estimate[2]),
          geom = "line",
          aes(color = "gamma"),
          size = .8) +
  stat_qq(distribution = qlnorm,
          dparams = list(lognorm_fit_mass_data01$estimate[1], lognorm_fit_mass_data01$estimate[2]),
          geom = "line",
          aes(color = "log normal"),
          size = .8) +
  scale_color_manual(name = "",
                     values = c("exponential" = "red",
                                "gamma" = "blue",
                                "log normal" = "green"),
                     breaks = c("exponential",
                                "gamma",
                                "log normal")) +
  labs(title = "Q-Q Plot Mass Data 01", x = "Theoretical Mass [Kg]", y = "Sample Mass [Kg]") +
  coord_fixed(clip = "off") +
  theme_dviz_open()
p3

# Q-Q Plot Data02
p4 <- ggplot(data02, aes(sample = mass)) + 
  geom_abline(slope = 1, intercept = 0, color = "grey70") +
  stat_qq(distribution = qexp,
          dparams = list(expo_fit_mass_data02$estimate[1]),
          geom = "line",
          aes(color = "exponential"),
          size = .8) +
  stat_qq(distribution = qgamma,
          dparams = list(gamma_fit_mass_data02$estimate[1], gamma_fit_mass_data02$estimate[2]),
          geom = "line",
          aes(color = "gamma"),
          size = .8) +
  stat_qq(distribution = qlnorm,
          dparams = list(lognorm_fit_mass_data02$estimate[1], lognorm_fit_mass_data02$estimate[2]),
          geom = "line",
          aes(color = "log normal"),
          size = .8) +
  scale_color_manual(name = "",
                     values = c("exponential" = "red",
                                "gamma" = "blue",
                                "log normal" = "green"),
                     breaks = c("exponential",
                                "gamma",
                                "log normal")) +
  labs(title = "Q-Q Plot Mass Data 02", x = "Theoretical Mass [Kg]", y = "Sample Mass [Kg]") +
  coord_fixed(clip = "off") +
  theme_dviz_open()
p4

```

### Speed Distribution
The difference in speed between the two analysed datasets shows us the rocks fall from differente hights. In dataset 01 mean speed is around 9Km./h, while in the other dataset we find that speed is far more spread: with the most ocurrences above 25 Km./h. 

```{r eda visualization plotting speed, warning=FALSE}

# Chart data01
p1 <- ggplot(data = data01, aes(x = speed)) + 
  geom_histogram(bins = 30, fill = "#22577a") +
  scale_y_continuous(name = "Count", expand = c(0, 0)) +
  scale_x_continuous(name = "Speed (m/s)", breaks = c(seq(0, 15, 3)), expand = c(0, 0)) + 
  labs(title = "Speed distribution in Dataset 01") +
  coord_cartesian(clip = "off") +
  theme_dviz_hgrid() +
  theme(
    axis.line.x = element_blank(),
    plot.margin = margin(3, 7, 3, 1.5)
  )

# Chart data02
p2 <- ggplot(data = data02, aes(x = speed)) + 
  geom_histogram(bins = 30, fill = "#22577a") +
  scale_y_continuous(name = "Count", expand = c(0, 0)) +
  scale_x_continuous(name = "Speed (m/s)", breaks = c(seq(0, 55, 5)), expand = c(0, 0)) + 
  labs(title = "Speed distribution in Dataset 02") +
  coord_cartesian(clip = "off") +
  theme_dviz_hgrid() +
  theme(
    axis.line.x = element_blank(),
    plot.margin = margin(3, 7, 3, 1.5)
  )

plot_grid(
  p1, NULL, p2,
  NULL, NULL, NULL,
  align = 'hv',
  rel_widths = c(1, .04, 1),
  rel_heights = c(1, .04, 1)
)

# Now we want to determine to what extent the observed data points do or do not follow a given distribution. We do that for our both datasets available.

# For data01
norm_fit_speed_data01 <- fitdistr(data01$speed, "normal")
logistic_fit_speed_data01 <- fitdistr(data01$speed, "logistic")

# For data02
norm_fit_speed_data02 <- fitdistr(data02$speed, "normal")
logistic_fit_speed_data02 <- fitdistr(data02$speed, "logistic")

# Q-Q Plot Data01
p3 <- ggplot(data01, aes(sample = speed)) + 
  geom_abline(slope = 1, intercept = 0, color = "grey70") +
  stat_qq(distribution = qnorm,
          dparams = list(norm_fit_speed_data01$estimate[1], norm_fit_speed_data01$estimate[2]),
          geom = "line",
          aes(color = "normal"),
          size = .8) +
  stat_qq(distribution = qlogis,
          dparams = list(logistic_fit_speed_data01$estimate[1], logistic_fit_speed_data01$estimate[2]),
          geom = "line",
          aes(color = "logistic"),
          size = .8) +
  scale_color_manual(name = "",
                     values = c("normal" = "red",
                                "logistic" = "blue"),
                     breaks = c("normal",
                                "logistic")) +
  labs(title = "Q-Q Plot Speed Data 01", x = "Theoretical Speed [m/s]", y = "Sample Speed [m/s]") +
  coord_fixed(clip = "off") +
  theme_dviz_open()
p3

# Q-Q Plot Data02
p4 <- ggplot(data02, aes(sample = speed)) + 
  geom_abline(slope = 1, intercept = 0, color = "grey70") +
  stat_qq(distribution = qnorm,
          dparams = list(norm_fit_speed_data02$estimate[1], norm_fit_speed_data02$estimate[2]),
          geom = "line",
          aes(color = "normal"),
          size = .8) +
  stat_qq(distribution = qlogis,
          dparams = list(logistic_fit_speed_data02$estimate[1], logistic_fit_speed_data02$estimate[2]),
          geom = "line",
          aes(color = "logistic"),
          size = .8) +
  scale_color_manual(name = "",
                     values = c("normal" = "red",
                                "logistic" = "blue"),
                     breaks = c("normal",
                                "logistic")) +
  labs(title = "Q-Q Plot Speed Data 02", x = "Theoretical Speed [m/s]", y = "Sample Speed [m/s]") +
  coord_fixed(clip = "off") +
  theme_dviz_open()
p4

```

### Energy Distribution
Again the differences between our two dataset are quite noticiable. Most rockfalls from heaviest boulders (dataset 01) turn less energy out, max. 50kJ., than the ones from lightest rocks: we find some of them up to 350kJ.

```{r eda visualization plotting energy}

# Chart data01
p1 <- ggplot(data = data01, aes(x = energy_kJ)) + 
  geom_histogram(bins = 30, fill = "#22577a") +
  scale_y_continuous(name = "Count", expand = c(0, 0)) +
  scale_x_continuous(name = "Energy (kJ)", breaks = c(seq(0, 200, 50)), expand = c(0, 0)) + 
  labs(title = "Energy distribution in Dataset 01") +
  coord_cartesian(clip = "off") +
  theme_dviz_hgrid() +
  theme(
    axis.line.x = element_blank(),
    plot.margin = margin(3, 7, 3, 1.5)
  )

# Chart data02
p2 <- ggplot(data = data02, aes(x = energy_kJ)) + 
  geom_histogram(bins = 30, fill = "#22577a") +
  scale_y_continuous(name = "Count", expand = c(0, 0)) +
  scale_x_continuous(name = "Energy (kJ)", breaks = c(seq(0, 450, 50)), expand = c(0, 0)) + 
  labs(title = "Energy distribution in Dataset 02") +
  coord_cartesian(clip = "off") +
  theme_dviz_hgrid() +
  theme(
    axis.line.x = element_blank(),
    plot.margin = margin(3, 7, 3, 1.5)
  )

plot_grid(
  p1, NULL, p2,
  NULL, NULL, NULL,
  align = 'hv',
  rel_widths = c(1, .04, 1),
  rel_heights = c(1, .04, 1)
)

```

### Mass/Speed/Energy relation
Here we show other way what we´ve already noticed above: heavier boulders fall down slower than lighter. Thus, correlation between mass and energy seems to be almost perfect linear. Individual points representation allows us to show how the events (rockfalls) clusters differently around certain x-axis values. In other words, how weight are the falling boulders.

```{r relation between variables}

# Chart mass/speed correlation data01
p1 <- ggplot(data = data01, mapping = aes(x = mass, y = speed)) +
  geom_smooth(mapping = aes(x = mass, y = speed), color = "#22577a", alpha = .2) +
  geom_point(color = "#92929e", size = 1.5) +
  scale_x_continuous(
    limits = c(0, 3200),
    expand = c(0, 0),
    name = "Mass (Kg)") +
  scale_y_continuous(
    limits = c(0, 20),
    expand = c(0, 0),
    name = "Speed (Km/h)"
  ) +
  labs(title = "Mass/Speed correl. Dataset01") +
  theme_dviz_open()

# Chart mass/speed correlation data02
p2 <- ggplot(data = data02, mapping = aes(x = mass, y = speed)) +
  geom_smooth(mapping = aes(x = mass, y = speed), color = "#22577a", alpha = .2) +
  geom_point(color = "#92929e", size = 1.5) +
  scale_x_continuous(
    limits = c(0, 420),
    expand = c(0, 0),
    name = "Mass (Kg)") +
  scale_y_continuous(
    limits = c(0, 50),
    expand = c(0, 0),
    name = "Speed (Km/h)"
  ) +
  labs(title = "Mass/Speed correl. Dataset02") +
  theme_dviz_open()

# Chart mass/energy correlation data01
p3 <- ggplot(data = data01, mapping = aes(x = mass, y = energy_kJ)) +
  geom_smooth(mapping = aes(x = mass, y = energy_kJ), color = "#22577a", alpha = .2) +
  geom_point(color = "#92929e", size = 1.5) +
  scale_x_continuous(
    limits = c(0, 3200),
    expand = c(0, 0),
    name = "Mass (Kg)") +
  scale_y_continuous(
    limits = c(0, 200),
    expand = c(0, 0),
    name = "Energy (kJ)"
  ) +
  labs(title = "Mass/Energy correl. Dataset01") +
  theme_dviz_open()

# Chart mass/energy correlation data02
p4 <- ggplot(data = data02, mapping = aes(x = mass, y = energy_kJ)) +
  geom_smooth(mapping = aes(x = mass, y = energy_kJ), color = "#22577a", alpha = .2) +
  geom_point(color = "#92929e", size = 1.5) +
  scale_x_continuous(
    limits = c(0, 420),
    expand = c(0, 0),
    name = "Mass (Kg)") +
  scale_y_continuous(
    limits = c(0, 400),
    expand = c(0, 0),
    name = "Energy (kJ)"
  ) +
  labs(title = "Mass/Energy correl. Dataset02") +
  theme_dviz_open()

plot_grid(
  p1, NULL, p2,
  NULL, NULL, NULL,
  p3, NULL, p4,
  align = 'hv',
  rel_widths = c(1, .04, 1),
  rel_heights = c(1, .04, 1)
)

```

Let´s bring together all boulders to have an overview.

```{r mass/speed and mass/energy scatterplots}

# Merge both datasets in one
df1_df2 <- rbind(data01, data02)

# Mass/Speed scatterplot
p1 <- ggplot(data = df1_df2, aes(x = speed, y = mass, size = mass)) +
  geom_point(color = "#22577a") +
  scale_x_continuous(name = "Speed (Km/h)") +
  scale_y_continuous(name = "Mass (Kg)") +
  scale_radius(
    name = "Boulder Mass (Kg)",
    guide = guide_legend(
      direction = "horizontal",
      title.position = "top",
      title.hjust = 0.9,
      label.position = "right",
      override.aes = list(fill = "#f0f0f2")
    )
  ) +
  labs(title = "Mass/Speed correlation all boulders") +
  theme_dviz_grid(12) +
  theme(
    legend.position = c(1, 0.9),
    legend.justification = c(1, 0),
    legend.spacing.x = unit(2, "pt"),
    legend.spacing.y = unit(2, "pt"),
    legend.background = element_rect(fill = "white", color = NA),
    legend.key.width = unit(10, "pt"),
    strip.text = element_text(size = 12, margin = margin(2, 0, 2, 0)),
    strip.background  = element_rect(
      fill = "#f0f0f2", colour = "#f0f0f2",
      linetype = 1, size = 0.25
    )
  )

# Mass/Energy scatterplot
p2 <- ggplot(data = df1_df2, aes(x = energy_kJ, y = mass, size = mass)) +
  geom_point(color = "#22577a") +
  scale_x_continuous(name = "Energy (kJ)") +
  scale_y_continuous(name = "Mass (Kg)") +
  scale_radius(
    name = "Boulder Mass (Kg)",
    guide = guide_legend(
      direction = "horizontal",
      title.position = "top",
      title.hjust = 0.9,
      label.position = "right",
      override.aes = list(fill = "#f0f0f2")
    )
  ) +
  labs(title = "Mass/Energy correlation all boulders") +
  theme_dviz_grid(12) +
  theme(
    legend.position = c(1, 0.9),
    legend.justification = c(1, 0),
    legend.spacing.x = unit(2, "pt"),
    legend.spacing.y = unit(2, "pt"),
    legend.background = element_rect(fill = "white", color = NA),
    legend.key.width = unit(10, "pt"),
    strip.text = element_text(size = 12, margin = margin(2, 0, 2, 0)),
    strip.background  = element_rect(
      fill = "#f0f0f2", colour = "#f0f0f2",
      linetype = 1, size = 0.25
    )
  )

plot_grid(
  p1, NULL, p2,
  NULL, NULL, NULL,
  align = 'hv',
  rel_widths = c(1, .04, 1),
  rel_heights = c(1, .04, 1)
)

```

### Rockfalls per Hour
Further, we want to know the distribution of rockfalls per hour from our total number of rockfalls (combined data01 and data02) in our sample period (from 2019-01-01 to 2019-03-27). At what time of day happens the most events?

```{r rockfalls hour}

# Count rockfalls per hour
rockfalls_hour <- count(df1_df2, time)
hightlight <- rockfalls_hour %>%
  mutate(tohighlight = ifelse(n %in% c(9, 8, 7), "yes", "no"))

# Total rockfalls per day
total_rockfalls <- sum(rockfalls_hour$n)

p <- ggplot(data = rockfalls_hour, aes(x = time, y = n)) +
  # geom_col(aes(fill = hightlight)) +
  # scale_fill_manual(
  #   values = c("#22577a", "red")) +
  geom_col(fill = "#22577a", alpha = 0.9) +
  scale_y_continuous(name = "Count",
                     expand = c(0, 0),
                     breaks = c(seq(0, 8, 2))) +
  scale_x_time(
    name = "Time of Day (24 Hours)",
    position = "bottom",
    breaks = unique(rockfalls_hour$time),
    labels = c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23")
  ) +
  geom_text(aes(label=n), vjust=1.5, colour="white") +
  labs(title = "Rockfalls per Hour", subtitle = paste("Total Rockfalls from 2019-01-01 to 2019-03-27 (sample period):", total_rockfalls)) +
  coord_cartesian(clip = "off") +
  theme_dviz_hgrid() +
  theme(
    axis.ticks.x = element_blank(),
    axis.line = element_blank(),
    plot.margin = margin(3, 7, 3, 1.5)
  )
p

```

### Traffic Density per Hour
Together with our rockfalls characteristics variables (mass, speed, enery and time), we need to consider data regarding the traffic density in order to carry our next probability calculations out. As the chart show us, no surprisingly the busiest traffic hours are between 16 and 19 pm.

```{r traffic density}

p <- ggplot(data = data_traffic) + 
  geom_step(
    mapping = aes(x = hour, y = pct_traffic),
    color = "#22577a",
    size = 0.75
  ) +
  scale_x_continuous(
    name = "Time of Day (24 Hours)",
    limits = c(0, 24),
    expand = c(0, 0),
    breaks = unique(data_traffic$hour)
  ) +
  scale_y_continuous(
    name = "Cumulative Frequency (%)",
    expand = c(0, 0), 
    limits = c(0, 9)
  ) +
  labs(title = "Traffic Density per Hour", subtitle = "Last data available: Mikrozensus Mobilität und Verkehr 2015") +
  coord_cartesian(clip = "off") +
  theme_dviz_grid() +
  theme(axis.line.x = element_blank())
p

```

# Probability
## Setting the Scene
First, we need to set up our scenario:

Given:
* Draped mesh fails stopping a rockfall event.
* Boulder mass over 2000 Kg.
* Cars per hour at [danger area](https://map.geo.admin.ch/?zoom=5&bgLayer=ch.swisstopo.pixelkarte-farbe&layers=ch.astra.strassenverkehrszaehlung_messstellen-regional_lokal,ch.astra.strassenverkehrszaehlung_messstellen-uebergeordnet&lang=de&topic=ech&E=2771057.86&N=1204031.93): 16'573 cars within 24 hours; steady traffic flow over the day without traffic jams. 
* Speed of cars: 60 Km/h or 16.66 m/s.
* Average [size of car](http://eupocketbook.org/tables/):
  * Length: 4.2m
  * Width: 1.8m

Premises:
* Boulders´ width and volume aren´t taking into account (=0).
* Car accidents caused by falling boulders (only +2000 Kg.) are always deadly.
* Only cars´ length is taking into consideration.
* Boulders´ energy (mass + speed) isn´t taking into account. Only the biggest boulders (+2000 Kg.) reach the road.
* For probability calculations a car would be hit by a falling boulder, we´re just taking into consideration the exactly moment when one boulder reachs the road.

Hypothesis:
A rockfall event happens, goes through the draped mesh and hits a car.

## Rockfall Probability per Hour
First, we want to know the probability of a rockfall takes place during the day.

```{r rockfall probability hour}

# Calculate expected value at a certain hour
(rockfalls_ev <- mutate(rockfalls_hour, exp_val_rockfall = n / 0.99 / 100))

# Let´s double check our results
sum(rockfalls_ev$exp_val_rockfall) # must be 1
sum(rockfalls_ev$n) # must be 99 (total of events registered in our sample)

```

## Danger Area
Considering our premises as starting point for our probability calculations, now we´re taking care of the time that a car remains in the so called danger area. In order to do that, we´re making use of car average speed and length and average width of boulders.

```{r car danger area}

# Total time (in seconds) one car remains in the danger area: distance (average car length) / speed (average speed 60 Km/h or 16.66 m/s)
time_car_da = 4.2 / 16.66

# Now that we have the total time a car remains in the danger area, we want to make this same calculation
# for all cars passing throught this zone during the day
total_car_per_hour <- mutate(
  data_traffic,
  cars_danger_zone_per_hour = pct_traffic * 165.73,
  time_danger_zone_seconds = cars_danger_zone_per_hour * time_car_da,
  exp_val_car_per_hour = time_danger_zone_seconds / 3600
)

# Now we calculate the expected value of a car getting hit by a boulder at a certain time
(
  exp_val_car_hit <- mutate(
    rockfalls_ev,
    events = rockfalls_ev$n,
    hour = total_car_per_hour$hour,
    cars_danger_zone_per_hour = total_car_per_hour$cars_danger_zone_per_hour,
    time_danger_zone_seconds = total_car_per_hour$time_danger_zone_seconds,
    exp_val_car = total_car_per_hour$exp_val_car_per_hour,
    total_exp_val = exp_val_rockfall * total_car_per_hour$exp_val_car_per_hour
  ) %>% # reorder the columns
    select(
      hour,
      events,
      cars_danger_zone_per_hour,
      time_danger_zone_seconds,
      exp_val_car,
      exp_val_rockfall,
      total_exp_val
    )
)

# Plotting our results
p <- ggplot(data = exp_val_car_hit) +
  geom_step(
    mapping = aes(x = hour, y = exp_val_rockfall, color = "Rockfall event"),
    size = 0.75
  ) +
  geom_step(
    mapping = aes(x = hour, y = exp_val_car, color = "Car driving through"),
    size = 0.75
  ) +
  geom_step(
    mapping = aes(x = hour, y = total_exp_val, color = "Boulder hit"),
    size = 0.75
  ) +
  scale_x_continuous(
    name = "Time of Day (24 Hours)",
    expand = c(0, 0),
    breaks = unique(exp_val_car_hit$hour)
  ) +
  scale_y_continuous(
    name = "Expected Value",
    expand = c(0, 0),
  ) +
  labs(title = "Expected Values") +
  coord_cartesian(clip = "off") +
  theme_dviz_grid() +
  theme(axis.line.x = element_blank()
)
p

```

## Probability of a Car Hit
After our data analysis and probability calculations, we end that the expected value of a car getting hit by a boulder at a certain point is as follows:

```{r prob car hit}

prob_car_hit <- sum(exp_val_car_hit$total_exp_val)
cat("The probability of a car getting hit by a falling boulder when it goes through the draped mesh and hits a car is:", prob_car_hit)

```

# Monte-Carlo Simulation
In order to select random samples from our probability distribution, we´re gonna make use of a Monte-Carlo Simulation. In our case, we want to build a Monte-Carlo simulation to count how many times a falling boulder goes through the draped mesh and hits a car taking into account numerous one-year simulations. Then we´re gonna to compare them each other.

To do that, three variables are created and measured:

* A __falling boulder event__: energy (kJ) is calculated as we did it before. If a falling boulder falls with more than 1000 kJ and if his cumulative weight in the draped mesh is more than 2000 Kg., then the boulder ends in the road.
 
* The __draped mesh clearing__: to avoid cumulative weight of fallen boulders breaks through the draped mesh, it´s cleared everyday at 08.00am.

* The _car hit event__: as for the calculation of the probability of car getting hit by a falling boulder, values of _traffic density per hour_ and _total number of cars per day_ come from the same main source: BfS.
 
```{r monte-carlo simulation}

# Falling boulder event
rock_event <- function(speed, mass){
  broken <- FALSE
  energy <- (.5 * mass * speed ** 2) / 1000
  if (energy >= 1000){
    broken <- TRUE
  } else if (energy >= 500){
    if (mass_in_net >= 2000){
      broken <- TRUE
    }
  }
  if (broken){
    num_rock_through_net <-  num_rock_through_net + 1
    events_current_year <- events_current_year + 1
  }
  mass_in_net  <-  mass_in_net + mass
  return(broken)
}

# Draped mesh clearing
net_clearing_process <- function(now, event_time_delta, clearing_time_minute, day_minutes){
  current_time <- now %% day_minutes
  event_time = current_time + event_time_delta
  if (current_time < clearing_time_minute){
    if (event_time >= clearing_time_minute){
      mass_in_net <- 0
    }
  } else {
    if (event_time >= (day_minutes + clearing_time_minute))
      mass_in_net <- 0
  }
}

# (expected_value_car <- mutate(exp_rock, time, n, expected_value,
#                           car_passing_hour = car_per_hour$cars_passing,
#                           t_car_in_danger = car_per_hour$time_in_danger,
#                           exp_car_per_hour = car_per_hour$exp_car,
#                           t_exp_per_hour = expected_value * car_per_hour$exp_car))
# 
# total_in_danger <- dplyr::select(expected_value_car, exp_car_per_hour)

# Car hit
car_hitting <- function(event_time, day_minutes){
  current_time <- event_time %% day_minutes
  #current_hour in integer, 8 O'clock = 08
  current_hour <- floor(current_time / 60)
  if (current_hour == 0) {
    current_hour = 24
  }
  #runif generates a random number between 0 and 1. If the number is below the expected value the car gets hit.
  hit_number <- runif(1, min = 0, max = 1)
  #Simulation with traffic data of the institute of statistics:
  if (hit_number <= exp_val_car_hit$exp_val_car[current_hour]) {
     car_hit_stat <- car_hit_stat + 1
  }

  #Simulation with evenly distributed traffic:
  if (hit_number <= 0.008166) {
    car_hit_even <- car_hit_even + 1
  }

  #Simulation with max traffic througout the day:
  if (hit_number <= 0.0175843437) {
    car_hit_max <- car_hit_max + 1
  }
  
}
    
#create dataframe for probability counter
# pro_counter <- NULL
# 

# Monte-Carlo simulation
monte_carlo_rockfall <- function(num_of_years, clearing_time_hour){
  
  day_minutes <- 24 * 60
  year_minutes <- day_minutes * 365
  clearing_time_minute <- clearing_time_hour * 60
  
  for (year in 1:num_of_years){
    current_year <- year
    events_current_year <- 0
    mass_in_net <- 0
    now <- 0
    time_to_next_event_out_1 <- 3732
    time_to_next_event_out_2 <- 3892
    while (now < year_minutes){
      
      if (time_to_next_event_out_1 < time_to_next_event_out_2){
        net_clearing_process(now, time_to_next_event_out_1, clearing_time_minute, day_minutes)
        time_to_next_event_out_2 <- time_to_next_event_out_2 - time_to_next_event_out_1
        now <- now + time_to_next_event_out_1
        time_to_next_event_out_1 <- 3746
        speed_1 <- rnorm(1, mean = nrm_fit_speed_out_1$estimate[1], sd = nrm_fit_speed_out_1$estimate[2])
        mass_1 <- rlnorm(1, meanlog = lnrm_fit_mass_out_1$estimate[1], sdlog = lnrm_fit_mass_out_1$estimate[2])
        broken <- rock_event(speed_1, mass_1)
        if (broken){
          car_hitting(now, day_minutes)
          break
        }

      } else {
        net_clearing_process(now, time_to_next_event_out_2, clearing_time_minute, day_minutes)
        time_to_next_event_out_1 <- time_to_next_event_out_1 - time_to_next_event_out_2
        now <- now + time_to_next_event_out_2
        time_to_next_event_out_2 <- rlnorm(1, meanlog = gDistTimeZ2$estimate[1], sdlog = gDistTimeZ2$estimate[2])
        speed_2 <- rnorm(1, mean = nrm_fit_speed_out_2$estimate[1], sd = nrm_fit_speed_out_2$estimate[2])
        mass_2 <- rexp(1, rate = e_fit_mass_out_2$estimate[1])
        broken <- rock_event(speed_2, mass_2)
        if (broken){
          car_hitting(now, day_minutes)
          break
        }
      }
    }
    yearly_prob_rock_through_net_mc <- num_rock_through_net / year
    yearly_prob_car_hit_stat_mc <- car_hit_stat / year
    yearly_prob_car_hit_even_my <- car_hit_even / year
    yearly_prob_car_hit_max_mc <- car_hit_max / year
    
      
    # pro_counter <- rbind(pro_counter, data.frame(current_year, events_current_year, num_rock_through_net, yearly_prob_rock_through_net_mc, yearly_prob_car_hit_stat_mc, yearly_prob_car_hit_even_my, yearly_prob_car_hit_max_mc ))
  }
  prob_rock_through_net_mc <- num_rock_through_net / num_of_years
  prob_car_hit_stat_mc <- car_hit_stat / num_of_years
  prob_car_hit_even_mc <- car_hit_even / num_of_years
  prob_car_hit_max_mc <- car_hit_max / num_of_years

  # file_pro_counter <- paste0('./RData/pro_counter_', num_of_years,'_years.rda')
  # save(pro_counter, file = file_pro_counter)

  # log_text <- paste('simulation_id:', simulation_id,
  #                   '\nrock through net: ', num_rock_through_net,
  #                   '\nprobability rock through net: ', prob_rock_through_net_mc,
  #                   '\ncar hit stat:', car_hit_stat, '(Calculated with traffic data of the swiss institute of statistics)',
  #                   '\nprobability car hit stat:', prob_car_hit_stat_mc,
  #                   '\ncar hit even:', car_hit_even, '(Calculated with an evenly distributed traffic)',
  #                   '\nprobability car hit even:', prob_car_hit_even_mc,
  #                   '\ncar hit max:', car_hit_max, '(Calculated with maximum traffic at all times)',
  #                   '\nprobability car hit max:', prob_car_hit_max_mc, '\n\n')

  # cat(log_text, file = "./Log/simulation.log", append = TRUE)

  # result_frame <- data.frame('simulation_id' = simulation_id,
  #                      'num_rock_through_net' = num_rock_through_net,
  #                      'prob_rock_through_net_mc' = prob_rock_through_net_mc,
  #                      'car_hit_stat' = car_hit_stat,
  #                      'prob_car_hit_stat_mc' = prob_car_hit_stat_mc,
  #                      'car_hit_even' = car_hit_even,
  #                      'prob_car_hit_even_mc' = prob_car_hit_even_mc,
  #                      'car_hit_max' = car_hit_max,
  #                      'prob_car_hit_max_mc' = prob_car_hit_max_mc,
  #                      'num_of_years' = num_of_years)
  return(result_frame)
  
}

```



# Linting

The code in this RMarkdown is linted with the [lintr package](https://github.com/jimhester/lintr), which is based on the  [tidyverse style guide](http://style.tidyverse.org/). 

```{r linting, echo=TRUE, message=FALSE, warning=FALSE}
lintr::lint("main.Rmd", linters =
              lintr::with_defaults(
                commented_code_linter = NULL,
                trailing_whitespace_linter = NULL
                )
            )
# if you have additional scripts and want them to be linted too, add them here
# lintr::lint("scripts/my_script.R")
```
